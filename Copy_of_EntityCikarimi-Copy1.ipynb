{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDb6ErnwV1Cd",
    "outputId": "9efa037b-c0b5-41f8-b577-5aae48f285b8"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "devide = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amUVgu_lOMfi",
    "outputId": "686151b1-b3aa-49f8-ea7a-bdf009fa8ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "# Later, load the dataset from disk\n",
    "dataset = load_dataset(\"wikiann\", \"tr\")\n",
    "\n",
    "# Verify that the dataset is loaded correctly\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sMOYsHoqr-Y5"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    106\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 107\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(inputs, targets)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=100, hidden_dim=200)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lTS_RJ54ayLA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2B4pRotWajDD"
   },
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "\n",
    "train_tokens = []\n",
    "train_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  train_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  train_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  test_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  test_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "df_train = pd.DataFrame({\"sentence\": train_tokens, \"tags\": train_tags})\n",
    "df_test = pd.DataFrame({\"sentence\": test_tokens, \"tags\": test_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "lfZZ3_tKaoMg",
    "outputId": "225a6213-172a-48e1-ecf8-9c8fd9747c6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.lük maçında Slovenya Millî Basketbol Takımı'...</td>\n",
       "      <td>O O entity entity entity entity O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>' '' Denizlispor '' '</td>\n",
       "      <td>O O entity O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hami Mandıralı 36 , Orhan Çıkırıkçı 46 , 48 , ...</td>\n",
       "      <td>entity entity O O entity entity O O O O entity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>San Antonio Spurs ( Milwaukee'den )</td>\n",
       "      <td>entity entity entity O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Divandere ( Dîwandere )</td>\n",
       "      <td>entity O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  3.lük maçında Slovenya Millî Basketbol Takımı'...   \n",
       "1                              ' '' Denizlispor '' '   \n",
       "2  Hami Mandıralı 36 , Orhan Çıkırıkçı 46 , 48 , ...   \n",
       "3                San Antonio Spurs ( Milwaukee'den )   \n",
       "4                            Divandere ( Dîwandere )   \n",
       "\n",
       "                                                tags  \n",
       "0  O O entity entity entity entity O O O O O O O ...  \n",
       "1                                     O O entity O O  \n",
       "2  entity entity O O entity entity O O O O entity...  \n",
       "3                         entity entity entity O O O  \n",
       "4                                       entity O O O  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AoePlS1jaoOy"
   },
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "text_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "label_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf-KZjeJaoYK"
   },
   "outputs": [],
   "source": [
    "from TorchCRF import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rHq9_yViagQN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenim şirketim kazanacaktır , onun adı da turkcell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mvocab\u001b[49m\u001b[38;5;241m.\u001b[39mget(word, vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m test_sentence\u001b[38;5;241m.\u001b[39msplit()], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m test_tensor \u001b[38;5;241m=\u001b[39m test_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "test_sentence = \"benim şirketim kazanacaktır , onun adı da turkcell\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long)\n",
    "test_tensor = test_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "best_tags\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejZbFBPLagWW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-JvGk6LagYs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNH4Bdp1agbD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWTrgIgVagdZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsddjEgDagfh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "text_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "label_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoAvjBVQagkN"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "# Later, load the dataset from disk\n",
    "dataset = load_dataset(\"wikiann\", \"tr\")\n",
    "\n",
    "# Verify that the dataset is loaded correctly\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "text_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "label_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hboPzcNPago3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=300, hidden_dim=600)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZEUUmhMagh1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(\"entity_cikarici.ai\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmCBXm7Gagmg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "devide = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"turkcell çok iyi bir hizmet ancak vodafone kotu\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long)\n",
    "test_tensor = test_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merhaba, ben Kerem Ali. İstanbul Teknik Üniversitesi Mesleki ve Teknik Anadolu Lisesi olarak katılıyoruz.\n",
    "Danışman hocam ile buraya geldim. Okulumun yapay zeka takımının kaptanı olarak katılıyorum. \n",
    "Sloganımız: Sorunlar bizim için şikayet değil proje konusudur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"türkcell çok iyi hizmet vodafone kötü\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model2(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6yCV3yyagq8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLUWAkDmagtT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Glp9Yy2qagvo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TJH44epagx-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS9Fgsg9ag0l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9nWBRNTag2r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W69vePCVag5B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtpy-pTyYR0H",
    "outputId": "bf0cb9cf-9943-4118-8241-6d064c5f24a7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'B-SAYI': 1, 'I-SAYI': 2, 'B-TARIH': 3, 'I-TARIH': 4, 'B-KISI': 5, 'I-KISI': 6}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında.\",\n",
    "    \"İstanbulun nüfusu yaklaşık 15 milyon kişidir.\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu.\",\n",
    "    \"Everest Dağının yüksekliği 8848 metredir.\",\n",
    "    \"2023 yılında Türkiye'nin nüfusu 84,6 milyon olarak tahmin ediliyor.\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    ['B-KISI', 'I-KISI', 'B-SAYI', 'I-SAYI', 'I-SAYI', 'O', 'O', 'O', 'O', 'B-SAYI', 'O', 'O'],  # Adjusted for length\n",
    "    ['O', 'O', 'O', 'B-SAYI', 'O', 'O'],  # Adjusted for length\n",
    "    ['B-KISI', 'I-KISI', 'I-KISI', 'B-TARIH', 'I-TARIH', 'I-TARIH', 'O', 'O', 'O'],  # Already correct\n",
    "    ['O', 'O', 'O', 'B-SAYI', 'O'],  # Adjusted for length\n",
    "    ['B-TARIH', 'O', 'O', 'O', 'B-SAYI', 'I-SAYI', 'O', 'O', 'O']  # Already correct\n",
    "]\n",
    "\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_random_date():\n",
    "    start_date = datetime(1900, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "    random_number_of_days = random.randrange(days_between_dates)\n",
    "    return start_date + timedelta(days=random_number_of_days)\n",
    "\n",
    "def generate_random_name():\n",
    "    first_names = [\"Ahmet\", \"Mehmet\", \"Ali\", \"Ayşe\", \"Fatma\", \"Mustafa\", \"Emine\", \"Hüseyin\", \"Hatice\", \"İbrahim\"]\n",
    "    last_names = [\"Yılmaz\", \"Kaya\", \"Demir\", \"Çelik\", \"Şahin\", \"Yıldız\", \"Öztürk\", \"Aydın\", \"Özdemir\", \"Arslan\"]\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "def generate_random_text():\n",
    "    templates = [\n",
    "        \"{} {} tarihinde doğdu ve şu an {} yaşında.\",\n",
    "        \"{}nin nüfusu yaklaşık {} milyon kişidir.\",\n",
    "        \"{} {} tarihinde {} kurdu.\",\n",
    "        \"{} Dağının yüksekliği {} metredir.\",\n",
    "        \"{} yılında Türkiye'nin nüfusu {} milyon olarak tahmin ediliyor.\"\n",
    "    ]\n",
    "\n",
    "    template = random.choice(templates)\n",
    "\n",
    "    if template == templates[0]:\n",
    "        name = generate_random_name()\n",
    "        birth_date = generate_random_date()\n",
    "        age = datetime.now().year - birth_date.year\n",
    "        return template.format(name, birth_date.strftime(\"%d %B %Y\"), age)\n",
    "    elif template == templates[1]:\n",
    "        cities = [\"İstanbul\", \"Ankara\", \"İzmir\", \"Bursa\", \"Antalya\"]\n",
    "        return template.format(random.choice(cities), random.randint(1, 20))\n",
    "    elif template == templates[2]:\n",
    "        events = [\"Türkiye Cumhuriyeti'ni\", \"TBMM'yi\", \"Türk Dil Kurumu'nu\"]\n",
    "        return template.format(generate_random_name(), generate_random_date().strftime(\"%d %B %Y\"), random.choice(events))\n",
    "    elif template == templates[3]:\n",
    "        mountains = [\"Everest\", \"K2\", \"Kangchenjunga\", \"Lhotse\", \"Makalu\"]\n",
    "        return template.format(random.choice(mountains), random.randint(5000, 9000))\n",
    "    else:\n",
    "        return template.format(random.randint(2023, 2050), random.uniform(80, 100))\n",
    "\n",
    "def generate_labels(text):\n",
    "    words = text.split()\n",
    "    labels = ['O'] * len(words)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word[0].isupper() and i > 0 and words[i-1][0].isupper():\n",
    "            labels[i-1] = 'B-KISI'\n",
    "            labels[i] = 'I-KISI'\n",
    "        elif word.isdigit():\n",
    "            if i > 0 and words[i-1] in [\"tarihinde\", \"yılında\"]:\n",
    "                labels[i] = 'B-TARIH'\n",
    "            else:\n",
    "                labels[i] = 'B-SAYI'\n",
    "        elif word in [\"Ocak\", \"Şubat\", \"Mart\", \"Nisan\", \"Mayıs\", \"Haziran\", \"Temmuz\", \"Ağustos\", \"Eylül\", \"Ekim\", \"Kasım\", \"Aralık\"]:\n",
    "            labels[i] = 'I-TARIH'\n",
    "\n",
    "    return labels\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for _ in range(100):\n",
    "    text = generate_random_text()\n",
    "    texts.append(text)\n",
    "    labels.append(generate_labels(text))\n",
    "\n",
    "# Print the results\n",
    "for i in range(100):\n",
    "    print(f\"Text: {texts[i]}\")\n",
    "    print(f\"Labels: {labels[i]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=100, hidden_dim=200)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "test_sentence = \"Ayşe Kaya 10 Nisan 2000'de 25 yaşına girecek.\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUE0prEWXBo7",
    "outputId": "9824cced-ed28-49b8-e0c6-fc4abe043653"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HGmBPltvX3m"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
