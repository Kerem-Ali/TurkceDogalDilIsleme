{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNHkeiOasHwA",
    "outputId": "acf638a4-91ae-4848-8350-50542573dccb"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchtext torchcrf pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDb6ErnwV1Cd",
    "outputId": "9efa037b-c0b5-41f8-b577-5aae48f285b8"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets==2.0.1\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCIP6_BYx7_b",
    "outputId": "a2f0a12f-b2d1-4a69-9079-9d469a61f576"
   },
   "outputs": [],
   "source": [
    "!pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSG8EPKiTCzc",
    "outputId": "e16cb30d-52fe-4c21-aa1f-f32c82e3fe03"
   },
   "outputs": [],
   "source": [
    "pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amUVgu_lOMfi",
    "outputId": "686151b1-b3aa-49f8-ea7a-bdf009fa8ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "# Later, load the dataset from disk\n",
    "dataset = load_dataset(\"wikiann\", \"tr\")\n",
    "\n",
    "# Verify that the dataset is loaded correctly\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lTS_RJ54ayLA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ai9a70Zga0-L",
    "outputId": "7bc2cfa2-9dd3-4783-9acd-19d814b39121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['3.lük', 'maçında', 'Slovenya', 'Millî', 'Basketbol', \"Takımı'nı\", 'yendikleri', 'maçta', '23', 'sayı', ',', '6', 'ribaund', ',', '2', 'blok', 'istatistikleriyle', 'oynamış', 've', '12', 'faul', 'yaptırmıştır', '.'], 'ner_tags': [0, 0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'langs': ['tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr', 'tr'], 'spans': [\"ORG: Slovenya Millî Basketbol Takımı'nı\"]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2B4pRotWajDD"
   },
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "\n",
    "train_tokens = []\n",
    "train_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  train_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  train_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  test_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  test_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "df_train = pd.DataFrame({\"sentence\": train_tokens, \"tags\": train_tags})\n",
    "df_test = pd.DataFrame({\"sentence\": test_tokens, \"tags\": test_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AoePlS1jaoOy"
   },
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "texts = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  texts.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  texts.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "labels = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  labels.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  labels.append([ner_encoding[a] for a in sample[\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvgUxhRoaoRI",
    "outputId": "81ff1f04-2de4-414a-aa01-01e708c3365a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "drVUSyDiaoTf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TorchCRF in /home/pc1/venv/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/pc1/venv/lib/python3.12/site-packages (from TorchCRF) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/pc1/venv/lib/python3.12/site-packages (from TorchCRF) (2.4.0)\n",
      "Requirement already satisfied: filelock in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (2024.5.0)\n",
      "Requirement already satisfied: setuptools in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (72.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->TorchCRF) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/pc1/venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->TorchCRF) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pc1/venv/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->TorchCRF) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pc1/venv/lib/python3.12/site-packages (from sympy->torch>=1.0.0->TorchCRF) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install TorchCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WeI6BMRRaoWG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchcrf in /home/pc1/venv/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/pc1/venv/lib/python3.12/site-packages (from torchcrf) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/pc1/venv/lib/python3.12/site-packages (from torchcrf) (2.4.0)\n",
      "Requirement already satisfied: filelock in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (2024.5.0)\n",
      "Requirement already satisfied: setuptools in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (72.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/pc1/venv/lib/python3.12/site-packages (from torch>=1.0.0->torchcrf) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/pc1/venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->torchcrf) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pc1/venv/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->torchcrf) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pc1/venv/lib/python3.12/site-packages (from sympy->torch>=1.0.0->torchcrf) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Vf-KZjeJaoYK"
   },
   "outputs": [],
   "source": [
    "from TorchCRF import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(inputs, targets)\n\u001b[1;32m     98\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 99\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:469\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    468\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 469\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/autograd/profiler.py:705\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 705\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/_ops.py:903\u001b[0m, in \u001b[0;36mTorchBindOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_register_as_effectful_op_temporarily():\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_dispatch_in_python(\n\u001b[1;32m    901\u001b[0m             args, kwargs, self_\u001b[38;5;241m.\u001b[39m_fallthrough_keys()\n\u001b[1;32m    902\u001b[0m         )\n\u001b[0;32m--> 903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "# Define NERDataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Define BiLSTM_CRF model class\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim=200, hidden_dim=256):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "# Example data (texts and labels should be extended with more diverse data)\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "# Prepare vocabulary and tag_to_idx\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model, optimizer, and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=400, hidden_dim=512).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)  # Adjusted learning rate\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Test sentence\n",
    "test_sentence = \"benim şirketim kazanacaktır, onun adı da turkcell\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "# Define NERDataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Define BiLSTM_CRF model class\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim=200, hidden_dim=256):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "# Example data (texts and labels should be extended with more diverse data)\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "# Prepare vocabulary and tag_to_idx\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model, optimizer, and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=800, hidden_dim=1024).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Adjusted learning rate\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Test sentence\n",
    "test_sentence = \"benim şirketim kazanacaktır, onun adı da turkcell\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cümlesi: Vatan Bilgisayardan cok iyi bir Türk Telekomdan daha iyi\n",
      "Tahmin edilen etiketler: ['entity', 'entity', 'O', 'O', 'O', 'entity', 'entity', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Vatan Bilgisayardan cok iyi bir Türk Telekomdan daha iyi\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cümlesi: bugün mediamarkt mağazasından iphone 7 plus satın aldım ama sorunlu çıktı\n",
      "Tahmin edilen etiketler: ['entity', 'entity', 'entity', 'entity', 'O', 'entity', 'entity', 'entity', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"bugün mediamarkt mağazasından iphone 7 plus satın aldım ama sorunlu çıktı\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(model,open(\"entity_cikarici.ai2\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc1/venv/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "model2 = pickle.load(open(\"entity_cikarici.ai\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cümlesi: bugün mediamarkt mağazasından iphone 7 plus satın aldım ama sorunlu çıktı\n",
      "Tahmin edilen etiketler: ['O', 'entity', 'entity', 'entity', 'entity', 'entity', 'O', 'entity', 'O', 'O', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc1/venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"bugün mediamarkt mağazasından iphone 7 plus satın aldım ama sorunlu çıktı\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model2(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zemberek-python in /home/pc1/venv/lib/python3.12/site-packages (0.2.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/pc1/venv/lib/python3.12/site-packages (from zemberek-python) (4.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/pc1/venv/lib/python3.12/site-packages (from zemberek-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install zemberek-python;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
