{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNHkeiOasHwA",
    "outputId": "acf638a4-91ae-4848-8350-50542573dccb"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchtext torchcrf pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDb6ErnwV1Cd",
    "outputId": "9efa037b-c0b5-41f8-b577-5aae48f285b8"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "wy5JCOGiV5XJ",
    "outputId": "b883a64e-cca1-40cd-b51c-3522c41ff87d"
   },
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/content/wikiann_tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "devide = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMOYsHoqr-Y5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=100, hidden_dim=200)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCIP6_BYx7_b",
    "outputId": "a2f0a12f-b2d1-4a69-9079-9d469a61f576"
   },
   "outputs": [],
   "source": [
    "!pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSG8EPKiTCzc",
    "outputId": "e16cb30d-52fe-4c21-aa1f-f32c82e3fe03"
   },
   "outputs": [],
   "source": [
    "pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amUVgu_lOMfi",
    "outputId": "686151b1-b3aa-49f8-ea7a-bdf009fa8ceb"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "# Later, load the dataset from disk\n",
    "dataset = load_dataset(\"wikiann\", \"tr\")\n",
    "\n",
    "# Verify that the dataset is loaded correctly\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTS_RJ54ayLA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ai9a70Zga0-L",
    "outputId": "7bc2cfa2-9dd3-4783-9acd-19d814b39121"
   },
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B4pRotWajDD"
   },
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "\n",
    "train_tokens = []\n",
    "train_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  train_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  train_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  test_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  test_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "df_train = pd.DataFrame({\"sentence\": train_tokens, \"tags\": train_tags})\n",
    "df_test = pd.DataFrame({\"sentence\": test_tokens, \"tags\": test_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "lfZZ3_tKaoMg",
    "outputId": "225a6213-172a-48e1-ecf8-9c8fd9747c6c"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoePlS1jaoOy"
   },
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "text_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "label_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvgUxhRoaoRI",
    "outputId": "81ff1f04-2de4-414a-aa01-01e708c3365a"
   },
   "outputs": [],
   "source": [
    "text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drVUSyDiaoTf"
   },
   "outputs": [],
   "source": [
    "pip install TorchCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeI6BMRRaoWG"
   },
   "outputs": [],
   "source": [
    "pip install torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf-KZjeJaoYK"
   },
   "outputs": [],
   "source": [
    "from TorchCRF import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ctm6IdVxS-M7",
    "outputId": "01cd4540-a099-4b9d-9429-6c8f85b3a949"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=300, hidden_dim=600)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rHq9_yViagQN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenim şirketim kazanacaktır onun adı da turkcell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mvocab\u001b[49m\u001b[38;5;241m.\u001b[39mget(word, vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m test_sentence\u001b[38;5;241m.\u001b[39msplit()], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m test_tensor \u001b[38;5;241m=\u001b[39m test_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "test_sentence = \"benim şirketim kazanacaktır onun adı da turkcell\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long)\n",
    "test_tensor = test_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "best_tags\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejZbFBPLagWW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-JvGk6LagYs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNH4Bdp1agbD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWTrgIgVagdZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsddjEgDagfh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m ner_encoding \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m6\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      3\u001b[0m text_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      5\u001b[0m   text_dataset\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "text_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "label_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoAvjBVQagkN"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "# Later, load the dataset from disk\n",
    "dataset = load_dataset(\"wikiann\", \"tr\")\n",
    "\n",
    "# Verify that the dataset is loaded correctly\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "text_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  text_dataset.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "label_dataset = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  label_dataset.append([ner_encoding[a] for a in sample[\"ner_tags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hboPzcNPago3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     65\u001b[0m     padded_labels \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([y, torch\u001b[38;5;241m.\u001b[39mzeros(max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(y), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)]) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(padded_texts), torch\u001b[38;5;241m.\u001b[39mstack(padded_labels)\n\u001b[1;32m     68\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAhmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mistanbul\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mun nüfusu yaklaşık 15 milyon kisidir\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMustafa Kemal Atatürk 29 Ekim 1923\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mte Türkiye Cumhuriyeti\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mni kurdu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity X\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min müşteri hizmetleri hızlı ve etkili Entity Y\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnin ürün kalitesi çok kötü\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 74\u001b[0m ] \u001b[38;5;241m+\u001b[39m \u001b[43mtext_dataset\u001b[49m\n\u001b[1;32m     76\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     78\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     82\u001b[0m ] \u001b[38;5;241m+\u001b[39m label_dataset\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Check and align lengths of texts and labels\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=300, hidden_dim=600)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0ZEUUmhMagh1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(\"entity_cikarici.ai\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tmCBXm7Gagmg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "devide = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 82\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(padded_texts), torch\u001b[38;5;241m.\u001b[39mstack(padded_labels)\n\u001b[1;32m     68\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAhmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mistanbul\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mun nüfusu yaklaşık 15 milyon kisidir\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m ] \u001b[38;5;241m+\u001b[39m text_dataset\n\u001b[1;32m     76\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     78\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     79\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     80\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     81\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 82\u001b[0m ] \u001b[38;5;241m+\u001b[39m \u001b[43mlabel_dataset\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Check and align lengths of texts and labels\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "texts = [\n",
    "    \"Ahmet Yılmaz 15 Mart 1990 tarihinde doğdu ve şu an 33 yaşında\",\n",
    "    \"istanbul'un nüfusu yaklaşık 15 milyon kisidir\",\n",
    "    \"Mustafa Kemal Atatürk 29 Ekim 1923'te Türkiye Cumhuriyeti'ni kurdu\",\n",
    "    \"Entity X'in müşteri hizmetleri hızlı ve etkili Entity Y'nin ürün kalitesi çok kötü\",\n",
    "    \"Fiber 100mb SuperOnline kullanıcısıyım yaklaşık 2 haftadır @Twitch @Kick_turkey gibi canlı yayın platformlarında 360p yayın izlerken donmalar yaşıyoruz başka hiç bir operatörler bu sorunu yaşamazken ben parasını verip alamadığım hizmeti neden ödeyeyim @Turkcell\"\n",
    "] + text_dataset\n",
    "\n",
    "labels = [\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'O', 'O', 'O', 'O','O'],\n",
    "    ['entity', 'entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "    ['entity', 'entity', 'O', 'O', 'O', 'O', 'O', 'entity', 'entity', 'O','O','O','O'],\n",
    "    ['O','O','entity','O','O','O','O','entity','entity','O','O','O','O','O','O','O','O','O','entity','entity','entity','entity','O','O','O','O','O','O','O','O','O','O','entity']\n",
    "] + label_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = BiLSTM_CRF(len(vocab), tag_to_idx, embedding_dim=300, hidden_dim=600)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "test_sentence = \"benim şirketim kazanacaktır , onun adı da turkcell\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long)\n",
    "test_tensor = test_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "best_tags\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenim şirketim kazanacaktır , onun adı da turkcell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mvocab\u001b[49m\u001b[38;5;241m.\u001b[39mget(word, vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m test_sentence\u001b[38;5;241m.\u001b[39msplit()], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m test_tensor \u001b[38;5;241m=\u001b[39m test_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "test_sentence = \"benim şirketim kazanacaktır , onun adı da turkcell\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long)\n",
    "test_tensor = test_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "best_tags\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6yCV3yyagq8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLUWAkDmagtT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Glp9Yy2qagvo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TJH44epagx-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS9Fgsg9ag0l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9nWBRNTag2r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W69vePCVag5B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtpy-pTyYR0H",
    "outputId": "bf0cb9cf-9943-4118-8241-6d064c5f24a7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUE0prEWXBo7",
    "outputId": "9824cced-ed28-49b8-e0c6-fc4abe043653"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HGmBPltvX3m"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
