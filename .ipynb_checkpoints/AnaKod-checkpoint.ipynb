{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca2b801-e17e-48c4-a252-f3c70f086ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 17:49:04.294258: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-08 17:49:04.303022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-08 17:49:04.313214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-08 17:49:04.316280: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-08 17:49:04.323998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-08 17:49:04.854771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723128546.589425    7733 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 17:49:06.610472: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metin: berqnet teoride guzel calisiyor fakat gercek dunyada calismasi uzun\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
      "Tahmin edilen duygu: pozitif\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Önceden eğitilmiş modeli yükleme\n",
    "with open(\"yuzde65sentiment.ai\", \"rb\") as f:\n",
    "    model_sentiment = pickle.load(f)\n",
    "\n",
    "# Tokenizer ve LabelEncoder'ı yükleme\n",
    "with open(\"tokenizer.pickle\", \"rb\") as f:\n",
    "    tokenizer_sentiment = pickle.load(f)\n",
    "with open(\"labelencoder.pickle\", \"rb\") as f:\n",
    "    le_sentiment = pickle.load(f)\n",
    "\n",
    "# Örnek kullanım\n",
    "durumlar = [\"negatif\",\"nötr\",\"pozitif\"]\n",
    "# Tahmin yapmak için fonksiyon\n",
    "def predict_sentiment(text):\n",
    "    global durumlar\n",
    "    sequence = tokenizer_sentiment.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=300)\n",
    "    prediction = model_sentiment.predict(padded)\n",
    "    return durumlar[le_sentiment.inverse_transform([np.argmax(prediction)])[0]]\n",
    "\n",
    "\n",
    "\n",
    "# Örnek kullanım\n",
    "sample_text = \"berqnet teoride guzel calisiyor fakat gercek dunyada calismasi uzun\"\n",
    "print(f\"Metin: {sample_text}\")\n",
    "pred = predict_sentiment(sample_text)\n",
    "print(f\"Tahmin edilen duygu: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f4ffd804-d26d-464b-95ee-079a3f69e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-08 20:02:43,320 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 2.846137285232544\n",
      "\n",
      "Generated Summary: berqnet teoride guzel calisiyor fakat gercek dunyada calismasi iyi\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Model seçimi ve eğitimi\n",
    "model_name = \"t5-small\"  # Daha büyük modeller için \"t5-base\" veya \"t5-large\" kullanabilirsiniz\n",
    "tokenizer_summary = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "import pickle\n",
    "model_summary = pickle.load(open(\"ilgilikisimcikarici.ai\",\"rb\"))\n",
    "\n",
    "def generate_summary(text):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model_summary.to(device)\n",
    "    inputs = tokenizer_summary(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    summary_ids = model_summary.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
    "    return tokenizer_summary.decode(summary_ids[0], skip_special_tokens=True)\n",
    "# Test\n",
    "test_sentence = \"berqnet teoride guzel calisiyor fakat gercek dunyada calismasi biraz sikintili .\"\n",
    "test_keyword = \"berqnet\"\n",
    "test_input = f\"Sentence: {test_sentence} Keyword: {test_keyword}\"\n",
    "\n",
    "generated_summary = generate_summary(test_input)\n",
    "print(f\"Generated Summary: {generated_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "571fbef0-3924-4ec1-869e-b5b2dd032d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for word in normalized.split():\\n    suggesteds = sc.suggest_for_word(word)\\n    if suggesteds:\\n        print(suggesteds)\\n        if word in suggesteds:\\n            print(word)\\n        else:\\n            print(suggesteds[0])'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = normalizer.normalize(generated_summary)\n",
    "normalized = normalized.replace(\".\",\" \")\n",
    "sc = TurkishSpellChecker(morphology)\n",
    "\"\"\"for word in normalized.split():\n",
    "    suggesteds = sc.suggest_for_word(word)\n",
    "    if suggesteds:\n",
    "        print(suggesteds)\n",
    "        if word in suggesteds:\n",
    "            print(word)\n",
    "        else:\n",
    "            print(suggesteds[0])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c51286ec-0ad1-417d-bc43-db79acef02fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.suggest_for_word(\"skntl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94019fa4-6aff-4827-b9c8-81ad8bfdba1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "165b4743-60af-49f9-a678-ff452a8d8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets==2.0.1\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Later, load the dataset from disk\n",
    "dataset = load_dataset(\"wikiann\", \"tr\")\n",
    "\n",
    "# Verify that the dataset is loaded correctly\n",
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "\n",
    "train_tokens = []\n",
    "train_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  train_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  train_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "test_tokens = []\n",
    "test_tags = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  test_tokens.append(' '.join(sample[\"tokens\"]))\n",
    "  test_tags.append(' '.join([ner_encoding[a] for a in sample[\"ner_tags\"]]))\n",
    "\n",
    "df_train = pd.DataFrame({\"sentence\": train_tokens, \"tags\": train_tags})\n",
    "df_test = pd.DataFrame({\"sentence\": test_tokens, \"tags\": test_tags})\n",
    "\n",
    "\n",
    "ner_encoding = {0: \"O\", 1: \"entity\", 2: \"entity\", 3: \"entity\", 4: \"entity\", 5: \"entity\", 6: \"entity\"}\n",
    "\n",
    "texts = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  texts.append(' '.join(sample[\"tokens\"]))\n",
    "for sample in dataset[\"test\"]:\n",
    "  texts.append(' '.join(sample[\"tokens\"]))\n",
    "\n",
    "labels = []\n",
    "for sample in dataset[\"train\"]:\n",
    "  labels.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "for sample in dataset[\"test\"]:\n",
    "  labels.append([ner_encoding[a] for a in sample[\"ner_tags\"]])\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "# Define NERDataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tag_to_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.vocab.get(word, self.vocab['<UNK>']) for word in self.texts[idx].split()]\n",
    "        labels = [self.tag_to_idx[tag] for tag in self.labels[idx]]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Define BiLSTM_CRF model class\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_idx, embedding_dim=200, hidden_dim=256):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.tagset_size = len(tag_to_idx)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embedded = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = (sentence != 0).bool()\n",
    "            loss = -self.crf(tag_scores, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            best_tags = self.crf.decode(tag_scores)\n",
    "            return best_tags\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(texts, labels):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_to_idx = {'O': 0, 'entity': 1}\n",
    "\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, tag_to_idx\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    padded_texts = [torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in texts]\n",
    "    padded_labels = [torch.cat([y, torch.zeros(max_len - len(y), dtype=torch.long)]) for y in labels]\n",
    "    return torch.stack(padded_texts), torch.stack(padded_labels)\n",
    "\n",
    "# Example data (texts and labels should be extended with more diverse data)\n",
    "\n",
    "\n",
    "# Check and align lengths of texts and labels\n",
    "for i in range(len(texts)):\n",
    "    text_length = len(texts[i].split())\n",
    "    label_length = len(labels[i])\n",
    "    if text_length != label_length:\n",
    "        raise ValueError(f\"Text and label lengths do not match at index {i}: {text_length} != {label_length}\")\n",
    "\n",
    "# Prepare vocabulary and tag_to_idx\n",
    "vocab, tag_to_idx = prepare_data(texts, labels)\n",
    "dataset = NERDataset(texts, labels, vocab, tag_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2cbfdcf7-e43a-476c-924c-41cd6b6f20f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity xin müşteri', 'entity ynin']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model, optimizer, and device\n",
    "import pickle\n",
    "model_entity = pickle.load(open(\"entity_cikarici_ellenmemis.ai\",\"rb\"))\n",
    "\n",
    "\n",
    "def extract_entities(sentence, tags):\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    \n",
    "    for word, tag in zip(sentence.split(), tags):\n",
    "        if tag == 'entity':\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(' '.join(current_entity))\n",
    "                current_entity = []\n",
    "    \n",
    "    # To handle the case where the last word(s) in the sentence are part of an entity\n",
    "    if current_entity:\n",
    "        entities.append(' '.join(current_entity))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_entities(text):\n",
    "    device = torch.device(\"cuda\")\n",
    "    test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in text.split()], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        best_tags = model_entity(test_tensor.unsqueeze(0))[0]\n",
    "    \n",
    "    idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "    predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "    words = text.split(\" \")\n",
    "    return extract_entities(text,predicted_labels)\n",
    "\n",
    "yorum = \"Entity Xin müşteri hizmetleri hızlı ve etkili Entity Ynin ürün kalitesi çok kötü\".lower()\n",
    "entities = get_entities(yorum)\n",
    "entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8c07138c-e9d8-4a1a-a762-fa9eafdc1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cümlesi: Vatan Bilgisayardan çok iyi bir Türk Telekomdan daha iyi\n",
      "Tahmin edilen etiketler: ['entity', 'entity', 'O', 'O', 'O', 'entity', 'entity', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "test_sentence = \"Vatan Bilgisayardan çok iyi bir Türk Telekomdan daha iyi\"\n",
    "test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in test_sentence.split()], dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    best_tags = model_entity(test_tensor.unsqueeze(0))[0]\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "print(\"Test cümlesi:\", test_sentence)\n",
    "print(\"Tahmin edilen etiketler:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "28165091-89ee-4b02-be05-3233977de77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(sentence, tags):\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    \n",
    "    for word, tag in zip(sentence.split(), tags):\n",
    "        if tag == 'entity':\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(' '.join(current_entity))\n",
    "                current_entity = []\n",
    "    \n",
    "    # To handle the case where the last word(s) in the sentence are part of an entity\n",
    "    if current_entity:\n",
    "        entities.append(' '.join(current_entity))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_entities(text):\n",
    "    device = torch.device(\"cuda\")\n",
    "    test_tensor = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in text.split()], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        best_tags = model_entity(test_tensor.unsqueeze(0))[0]\n",
    "    \n",
    "    idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "    predicted_labels = [idx_to_tag[i] for i in best_tags]\n",
    "    words = text.split(\" \")\n",
    "    return extract_entities(text,predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "833285f8-ad30-42d8-b845-8da4d40667f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entity Xin müşteri', 'Entity Ynin']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yorum = \"Entity Xin müşteri hizmetleri hızlı ve etkili Entity Ynin ürün kalitesi çok kötü\"\n",
    "entities = get_entities(yorum)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b50c5232-54b9-4e54-a9a9-bb56c61a3f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Xin müşteri\n",
      "Entity Xin müşteri hizmetleri hzl ve etkili Entity Ynin ürün kalitesi çok.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "pozitif\n",
      "--------------------------------------------------\n",
      "Entity Ynin\n",
      "Entité Xin müşteri hizmetleri hzl ve etkili Entity Ynin ürün kalitesi çok kötü\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "negatif\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for entity in entities:\n",
    "    test_input = f\"Sentence: {yorum} Keyword: {entity}\"\n",
    "    ilgili_kisim = generate_summary(test_input)\n",
    "    print(entity)\n",
    "    print(ilgili_kisim)\n",
    "    print(predict_sentiment(ilgili_kisim))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865879b-8f69-4e54-afc9-ea10c10bcbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536176c-fb98-425e-9bf3-624431de14ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
